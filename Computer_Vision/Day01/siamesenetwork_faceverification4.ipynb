{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedshouaib/iti/blob/main/Computer_Vision/Day01/siamesenetwork_faceverification4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-02T08:35:02.563543Z",
          "iopub.status.busy": "2022-04-02T08:35:02.563198Z",
          "iopub.status.idle": "2022-04-02T08:35:08.798471Z",
          "shell.execute_reply": "2022-04-02T08:35:08.797717Z",
          "shell.execute_reply.started": "2022-04-02T08:35:02.563479Z"
        },
        "id": "E_Qf6ev236Hm",
        "outputId": "f6feed8c-235f-4586-9fb6-01eab1704a6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'iti'...\n",
            "remote: Enumerating objects: 668, done.\u001b[K\n",
            "remote: Counting objects: 100% (453/453), done.\u001b[K\n",
            "remote: Compressing objects: 100% (444/444), done.\u001b[K\n",
            "remote: Total 668 (delta 86), reused 0 (delta 0), pack-reused 215 (from 1)\u001b[K\n",
            "Receiving objects: 100% (668/668), 27.23 MiB | 21.28 MiB/s, done.\n",
            "Resolving deltas: 100% (158/158), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mohamedshouaib/iti.git\n",
        "!cd iti/Computer_Vision/Day01/Siamese"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "2_U5_fzuAD0b"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper Parameters\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = (150, 150)"
      ],
      "metadata": {
        "id": "YVYbGlpaic-m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Loading Functions (Your existing code)\n",
        "def load_dataset(base_path=\"iti/Computer_Vision/Day01/Siamese\"):\n",
        "    data = {'train': {}, 'test': {}}\n",
        "\n",
        "    for person in os.listdir(base_path):\n",
        "        person_path = os.path.join(base_path, person)\n",
        "        if not os.path.isdir(person_path):\n",
        "            continue\n",
        "\n",
        "        for split in ['Train', 'Test']:\n",
        "            split_path = os.path.join(person_path, split)\n",
        "            if not os.path.exists(split_path):\n",
        "                print(f\"Missing {split} folder for {person}\")\n",
        "                continue\n",
        "\n",
        "            csv_files = [f for f in os.listdir(split_path) if f.endswith('.csv')]\n",
        "            if not csv_files:\n",
        "                print(f\"No CSV found in {split_path}\")\n",
        "                continue\n",
        "\n",
        "            csv_path = os.path.join(split_path, csv_files[0])\n",
        "\n",
        "            genuine = []\n",
        "            forged = []\n",
        "\n",
        "            with open(csv_path, 'r') as f:\n",
        "                try:\n",
        "                    reader = csv.DictReader(f)\n",
        "                    row = next(reader)\n",
        "\n",
        "                    img_col = None\n",
        "                    label_col = None\n",
        "\n",
        "                    for col in row.keys():\n",
        "                        col_lower = col.lower()\n",
        "                        if 'image' in col_lower or 'name' in col_lower:\n",
        "                            img_col = col\n",
        "                        elif 'label' in col_lower or 'class' in col_lower:\n",
        "                            label_col = col\n",
        "\n",
        "                    if not img_col or not label_col:\n",
        "                        raise ValueError(\"Couldn't detect required columns\")\n",
        "\n",
        "                    f.seek(0)\n",
        "                    next(reader)\n",
        "\n",
        "                    for row in reader:\n",
        "                        img_name = row[img_col].strip()\n",
        "                        img_path = os.path.join(split_path, img_name)\n",
        "\n",
        "                        if not os.path.exists(img_path):\n",
        "                            print(f\"Missing image: {img_path}\")\n",
        "                            continue\n",
        "\n",
        "                        label = row[label_col].strip().lower()\n",
        "                        if label == 'real' or label == 'genuine':\n",
        "                            genuine.append(img_path)\n",
        "                        elif label == 'forged' or label == 'fake':\n",
        "                            forged.append(img_path)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {csv_path}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if genuine or forged:\n",
        "                data[split.lower()][person] = {\n",
        "                    'genuine': genuine,\n",
        "                    'forged': forged\n",
        "                }\n",
        "\n",
        "    return data['train'], data['test']"
      ],
      "metadata": {
        "id": "e47yhiSxIHIl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_triplets(data_dict, split='train'):\n",
        "    triplets = []\n",
        "    persons = list(data_dict[split].keys())\n",
        "\n",
        "    for person in persons:\n",
        "        genuine = data_dict[split][person]['genuine']\n",
        "        forged = data_dict[split][person]['forged']\n",
        "\n",
        "        for i in range(len(genuine)):\n",
        "            for j in range(i+1, len(genuine)):\n",
        "                anchor = genuine[i]\n",
        "                positive = genuine[j]\n",
        "\n",
        "                for neg in forged:\n",
        "                    triplets.append((anchor, positive, neg))\n",
        "\n",
        "    return triplets"
      ],
      "metadata": {
        "id": "7-Dk4UXBcZno"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset Class\n",
        "class SignatureTripletDataset(Dataset):\n",
        "    def __init__(self, triplets, transform=None):\n",
        "        self.triplets = triplets\n",
        "        self.transform = transform or transforms.Compose([\n",
        "            transforms.Resize(IMAGE_SIZE),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_path, positive_path, negative_path = self.triplets[idx]\n",
        "\n",
        "        anchor = Image.open(anchor_path).convert('L')\n",
        "        positive = Image.open(positive_path).convert('L')\n",
        "        negative = Image.open(negative_path).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            anchor = self.transform(anchor)\n",
        "            positive = self.transform(positive)\n",
        "            negative = self.transform(negative)\n",
        "\n",
        "        return anchor, positive, negative"
      ],
      "metadata": {
        "id": "h5_hTUtMn1Pe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Architecture\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.3),  # Added dropout\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.3),  # Added dropout\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.3),  # Added dropout\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Dropout(0.5),  # Higher dropout before final layers\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512*18*18, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),  # Dropout in fully connected layer\n",
        "            nn.Linear(1024, 256)  # Smaller embedding size\n",
        "        )\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        return self.cnn(x)\n",
        "\n",
        "    def forward(self, input1, input2, input3):\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "        output3 = self.forward_once(input3)\n",
        "        return output1, output2, output3"
      ],
      "metadata": {
        "id": "uW9yOXJ8nmYn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Triplet Loss\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        pos_dist = F.pairwise_distance(anchor, positive, 2)\n",
        "        neg_dist = F.pairwise_distance(anchor, negative, 2)\n",
        "        losses = F.relu(pos_dist - neg_dist + self.margin)\n",
        "        return losses.mean()"
      ],
      "metadata": {
        "id": "YVh3zCDpSph3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, test_loader, args):\n",
        "    model = SiameseNetwork()\n",
        "    if args.cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    # Initialize Triplet Loss criterion with margin from args\n",
        "    criterion = TripletLoss(margin=args.margin)\n",
        "\n",
        "    # Optimizer with L2 regularization\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', patience=2, factor=0.5, verbose=True\n",
        "    )\n",
        "\n",
        "    # Early stopping setup\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_train_loss = 0.0\n",
        "\n",
        "        with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{args.epochs} [Train]\") as tepoch:\n",
        "            for anchor, positive, negative in tepoch:\n",
        "                if args.cuda:\n",
        "                    anchor, positive, negative = anchor.cuda(), positive.cuda(), negative.cuda()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                anchor_out, pos_out, neg_out = model(anchor, positive, negative)\n",
        "                loss = criterion(anchor_out, pos_out, neg_out)\n",
        "\n",
        "                loss.backward()\n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_train_loss += loss.item()\n",
        "                tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with tqdm(test_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{args.epochs} [Val]\") as vepoch:\n",
        "                for anchor, positive, negative in vepoch:\n",
        "                    if args.cuda:\n",
        "                        anchor, positive, negative = anchor.cuda(), positive.cuda(), negative.cuda()\n",
        "\n",
        "                    anchor_out, pos_out, neg_out = model(anchor, positive, negative)\n",
        "                    val_loss = criterion(anchor_out, pos_out, neg_out)\n",
        "                    epoch_val_loss += val_loss.item()\n",
        "                    vepoch.set_postfix(loss=val_loss.item())\n",
        "\n",
        "        avg_val_loss = epoch_val_loss / len(test_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Update learning rate based on validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "        print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "      # Plot training history\n",
        "      plt.figure(figsize=(10, 5))\n",
        "      plt.plot(train_losses, label='Training Loss')\n",
        "      plt.plot(val_losses, label='Validation Loss')\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.title('Training and Validation Loss')\n",
        "      plt.legend()\n",
        "      plt.savefig('training_history.png')\n",
        "      plt.close()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "y2prKGo9Yohl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    epochs = 5\n",
        "    margin = 1.0\n",
        "    cuda = torch.cuda.is_available()\n",
        "\n",
        "args = Args()\n"
      ],
      "metadata": {
        "id": "pkZP3bY8PqnR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "print(\"Loading dataset...\")\n",
        "train_data, test_data = load_dataset()\n",
        "print(\"Generating triplets...\")\n",
        "train_triplets = generate_triplets({'train': train_data, 'test': test_data}, 'train')\n",
        "test_triplets = generate_triplets({'train': train_data, 'test': test_data}, 'test')\n",
        "\n",
        "print(f\"Training triplets: {len(train_triplets)}\")\n",
        "print(f\"Testing triplets: {len(test_triplets)}\")\n",
        "\n",
        "# Create datasets\n",
        "print(\"Creating datasets...\")\n",
        "train_dataset = SignatureTripletDataset(train_triplets)\n",
        "test_dataset = SignatureTripletDataset(test_triplets)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Train model\n",
        "print(\"Starting training...\")\n",
        "model = train_model(train_loader, test_loader, args)\n",
        "\n",
        "# Save final model\n",
        "torch.save(model.state_dict(), 'final_model.pth')\n",
        "print(\"Training complete. Models saved as 'best_model.pth' and 'final_model.pth'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtWMpSBvIHXx",
        "outputId": "0c96092c-4ef8-427a-ab15-226ca7f2b287"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Generating triplets...\n",
            "Training triplets: 19000\n",
            "Testing triplets: 120\n",
            "Creating datasets...\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Epoch 1/5 [Train]: 100%|██████████| 594/594 [06:42<00:00,  1.48batch/s, loss=0]\n",
            "Epoch 1/5 [Val]: 100%|██████████| 4/4 [00:01<00:00,  2.68batch/s, loss=0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Summary:\n",
            "Train Loss: 0.0337 | Val Loss: 2.0138\n",
            "Learning Rate: 1.00e-04\n",
            "Best Val Loss: 2.0138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5 [Train]: 100%|██████████| 594/594 [06:36<00:00,  1.50batch/s, loss=0]\n",
            "Epoch 2/5 [Val]: 100%|██████████| 4/4 [00:01<00:00,  2.54batch/s, loss=0.204]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Summary:\n",
            "Train Loss: 0.0074 | Val Loss: 0.8951\n",
            "Learning Rate: 1.00e-04\n",
            "Best Val Loss: 0.8951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5 [Train]: 100%|██████████| 594/594 [06:35<00:00,  1.50batch/s, loss=0]\n",
            "Epoch 3/5 [Val]: 100%|██████████| 4/4 [00:01<00:00,  2.70batch/s, loss=0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 Summary:\n",
            "Train Loss: 0.0044 | Val Loss: 2.4283\n",
            "Learning Rate: 1.00e-04\n",
            "Best Val Loss: 0.8951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5 [Train]: 100%|██████████| 594/594 [06:35<00:00,  1.50batch/s, loss=0]\n",
            "Epoch 4/5 [Val]: 100%|██████████| 4/4 [00:01<00:00,  2.64batch/s, loss=0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 Summary:\n",
            "Train Loss: 0.0076 | Val Loss: 1.7476\n",
            "Learning Rate: 1.00e-04\n",
            "Best Val Loss: 0.8951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5 [Train]: 100%|██████████| 594/594 [06:35<00:00,  1.50batch/s, loss=0]\n",
            "Epoch 5/5 [Val]: 100%|██████████| 4/4 [00:01<00:00,  2.67batch/s, loss=0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping triggered at epoch 5\n",
            "Training complete. Models saved as 'best_model.pth' and 'final_model.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_triplet_images(triplet, img_size=(150, 150)):\n",
        "    def load_image(path):\n",
        "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            raise ValueError(f\"Failed to load image: {path}\")\n",
        "        img = cv2.resize(img, img_size)\n",
        "        return img.astype(np.float32) / 255.0  # Normalize to [0,1]\n",
        "\n",
        "    anchor = load_image(triplet[0])\n",
        "    positive = load_image(triplet[1])\n",
        "    negative = load_image(triplet[2])\n",
        "\n",
        "    return anchor, positive, negative"
      ],
      "metadata": {
        "id": "9SaOgzWqjJPV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_triplets):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for triplet in test_triplets:\n",
        "            a, p, n = load_triplet_images(triplet)\n",
        "            a = torch.tensor(a).unsqueeze(0).unsqueeze(0).to(device)\n",
        "            p = torch.tensor(p).unsqueeze(0).unsqueeze(0).to(device)\n",
        "            n = torch.tensor(n).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "            out_a, out_p = model.forward_once(a), model.forward_once(p)\n",
        "            out_n = model.forward_once(n)\n",
        "\n",
        "            d_ap = F.pairwise_distance(out_a, out_p)\n",
        "            d_an = F.pairwise_distance(out_a, out_n)\n",
        "\n",
        "            if d_ap.item() < d_an.item():\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    print(f\"Accuracy on test set: {correct / total:.2f}\")\n"
      ],
      "metadata": {
        "id": "b1471jzukG-v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on the test triplets\n",
        "evaluate_model(model, test_triplets)"
      ],
      "metadata": {
        "id": "QE9sLNWtkHBV",
        "outputId": "2f3bc859-c7dc-4325-8e24-38075ac061cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'device' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-980abed85feb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate model on the test triplets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_triplets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-6a6d0bdae818>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_triplets)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtriplet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_triplets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_triplet_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'siamese_signature_model.pth')\n",
        "\n",
        "# To load later\n",
        "# model.load_state_dict(torch.load('siamese_signature_model.pth'))\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "x85_Z--nkHGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def test_signature_pair(img_path1, img_path2, model, threshold=1.5):\n",
        "    model.eval()\n",
        "    img1, img2 = load_triplet_images((img_path1, img_path2, img_path2))[:2]  # ignore negative\n",
        "\n",
        "    img1 = torch.tensor(img1).unsqueeze(0).unsqueeze(0).to(device)\n",
        "    img2 = torch.tensor(img2).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out1 = model.forward_once(img1)\n",
        "        out2 = model.forward_once(img2)\n",
        "\n",
        "    distance = F.pairwise_distance(out1, out2).item()\n",
        "    print(f\"Distance: {distance:.4f}\")\n",
        "    return distance < threshold  # True if similar"
      ],
      "metadata": {
        "id": "j2nuTC1skHH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example real vs. real (should return True if genuine match)\n",
        "img_path1 = 'iti/Computer_Vision/Day01/Siamese/personA/Test/personA_29.png'\n",
        "img_path2 = 'iti/Computer_Vision/Day01/Siamese/personA/Test/personA_13.png'\n",
        "\n",
        "# Example real vs. forged (should return False if forged)\n",
        "img_path3 = 'iti/Computer_Vision/Day01/Siamese/personA/Test/personA_29.png'\n",
        "img_path4 = 'iti/Computer_Vision/Day01/Siamese/personA/Test/personA_33.png'\n",
        "\n",
        "# Test genuine match\n",
        "is_match = test_signature_pair(img_path1, img_path2, model)\n",
        "print(\"Match (real vs. real):\", is_match)\n",
        "\n",
        "# Test forged case\n",
        "is_forged = test_signature_pair(img_path3, img_path4, model)\n",
        "print(\"Match (real vs. forged):\", is_forged)"
      ],
      "metadata": {
        "id": "r0WxUkftkHJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################"
      ],
      "metadata": {
        "id": "jCCAbWnMjJf8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}