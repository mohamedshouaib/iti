{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-YCgF-9MN_dn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import spacy\n",
        "import sacrebleu\n",
        "import math\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set device"
      ],
      "metadata": {
        "id": "T-4WfVbanbWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhTH5B4nOPGZ",
        "outputId": "98ef4fe8-ee66-4539-9976-f78cf51684c0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "jZ-JPBmpOUpy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load spaCy models"
      ],
      "metadata": {
        "id": "-iAuvb1Pnead"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "    spacy_ger = spacy.load(\"de_core_news_sm\")\n",
        "    print(\"spaCy models loaded successfully.\")\n",
        "except:\n",
        "    print(\"Installing spaCy models...\")\n",
        "    import os\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "    os.system(\"python -m spacy download de_core_news_sm\")\n",
        "    spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "    spacy_ger = spacy.load(\"de_core_news_sm\")\n",
        "    print(\"spaCy models installed and loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31vzHqPHOWnv",
        "outputId": "35ff2d14-1578-4a48-c04f-b91c79db9bef"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy models loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization functions"
      ],
      "metadata": {
        "id": "bA2CF1lKngxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_en(text):\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text.lower() for tok in spacy_ger.tokenizer(text)]"
      ],
      "metadata": {
        "id": "BrjUiW__OZfP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocabulary building function"
      ],
      "metadata": {
        "id": "0XJLQAzuniZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sentences, tokenizer, min_freq=2, max_size=7000):\n",
        "    counter = Counter()\n",
        "    for s in sentences:\n",
        "        counter.update(tokenizer(s))\n",
        "    vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "    words_and_frequencies = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "    for word, freq in words_and_frequencies:\n",
        "        if freq >= min_freq:\n",
        "            if len(vocab) >= max_size:\n",
        "                break\n",
        "            vocab[word] = len(vocab)\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "xQP2EWiLOdDZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and preprocess dataset\n"
      ],
      "metadata": {
        "id": "4uW3r1w9nkJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"opus_books\", \"de-en\")\n",
        "train_data = dataset['train']\n",
        "en_sentences = [ex['translation']['en'] for ex in train_data]\n",
        "de_sentences = [ex['translation']['de'] for ex in train_data]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lZoWkexOfzC",
        "outputId": "82b02ba2-1a33-4cad-f77b-4582cbed30f4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building vocabularies...\")\n",
        "en_vocab = build_vocab(en_sentences, tokenize_en, min_freq=2)\n",
        "de_vocab = build_vocab(de_sentences, tokenize_de, min_freq=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K_WvP7jOkjK",
        "outputId": "26b7c59d-f144-49a0-f574-02e36b5daab2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building vocabularies...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"English vocabulary size: {len(en_vocab)}\")\n",
        "print(f\"German vocabulary size: {len(de_vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x0BjvPsOmY9",
        "outputId": "90d4af2a-55a8-41b2-f98f-468a8215acb6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary size: 7000\n",
            "German vocabulary size: 7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset class\n"
      ],
      "metadata": {
        "id": "w0eDxRW0nmyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, data_pairs, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer, max_len=100):\n",
        "        self.data_pairs = []\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        for pair in data_pairs:\n",
        "            src_tokens = src_tokenizer(pair['en'])\n",
        "            tgt_tokens = tgt_tokenizer(pair['de'])\n",
        "            if len(src_tokens) <= max_len and len(tgt_tokens) <= max_len:\n",
        "                self.data_pairs.append(pair)\n",
        "        print(f\"Retained {len(self.data_pairs)} of {len(data_pairs)} examples after length filtering\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.data_pairs[idx]\n",
        "        src_tokens = self.src_tokenizer(pair['en'])\n",
        "        src_indices = [self.src_vocab.get(token, self.src_vocab['<UNK>']) for token in src_tokens]\n",
        "        src_indices = [self.src_vocab['<SOS>']] + src_indices + [self.src_vocab['<EOS>']]\n",
        "        src_tensor = torch.LongTensor(src_indices)\n",
        "\n",
        "        tgt_tokens = self.tgt_tokenizer(pair['de'])\n",
        "        tgt_indices = [self.tgt_vocab.get(token, self.tgt_vocab['<UNK>']) for token in tgt_tokens]\n",
        "        tgt_indices = [self.tgt_vocab['<SOS>']] + tgt_indices + [self.tgt_vocab['<EOS>']]\n",
        "        tgt_tensor = torch.LongTensor(tgt_indices)\n",
        "\n",
        "        return src_tensor, tgt_tensor"
      ],
      "metadata": {
        "id": "3wvcbtwGO-Pz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collate function\n"
      ],
      "metadata": {
        "id": "vOqa9Oponom0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    src_seqs, tgt_seqs = zip(*batch)\n",
        "    src_lengths = torch.LongTensor([len(s) for s in src_seqs])\n",
        "    tgt_lengths = torch.LongTensor([len(t) for t in tgt_seqs])\n",
        "    src_padded = pad_sequence(src_seqs, batch_first=True, padding_value=0)\n",
        "    tgt_padded = pad_sequence(tgt_seqs, batch_first=True, padding_value=0)\n",
        "    return src_padded, tgt_padded, src_lengths, tgt_lengths"
      ],
      "metadata": {
        "id": "WjVW7-7CPA_w"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create datasets and dataloaders\n"
      ],
      "metadata": {
        "id": "1cdhlQApnqVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_pairs = [{'en': en, 'de': de} for en, de in zip(en_sentences, de_sentences)]\n",
        "train_size = min(30000, len(data_pairs))\n",
        "val_size = min(3000, len(data_pairs) - train_size)\n",
        "indices = list(range(len(data_pairs)))\n",
        "random.shuffle(indices)\n",
        "train_indices = indices[:train_size]\n",
        "val_indices = indices[train_size:train_size+val_size]\n",
        "train_pairs = [data_pairs[i] for i in train_indices]\n",
        "val_pairs = [data_pairs[i] for i in val_indices]"
      ],
      "metadata": {
        "id": "fO-WKPFIPFHx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TranslationDataset(train_pairs, en_vocab, de_vocab, tokenize_en, tokenize_de, max_len=80)\n",
        "val_dataset = TranslationDataset(val_pairs, en_vocab, de_vocab, tokenize_en, tokenize_de, max_len=80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-CuPa7kPF0w",
        "outputId": "7972760f-0595-47a5-d855-cd127184da64"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retained 29151 of 30000 examples after length filtering\n",
            "Retained 2907 of 3000 examples after length filtering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "k156N409PIGS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder class\n"
      ],
      "metadata": {
        "id": "LrlxHOCMnsK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, bidirectional=True, dropout=dropout if n_layers > 1 else 0, batch_first=True)\n",
        "        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.fc_cell = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        packed_embedded = pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True)\n",
        "        packed_outputs, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "        hidden_processed = []\n",
        "        cell_processed = []\n",
        "        for layer in range(self.n_layers):\n",
        "            forward_hidden = hidden[2*layer]\n",
        "            backward_hidden = hidden[2*layer+1]\n",
        "            combined_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
        "            processed_hidden = torch.tanh(self.fc_hidden(combined_hidden))\n",
        "            hidden_processed.append(processed_hidden)\n",
        "\n",
        "            forward_cell = cell[2*layer]\n",
        "            backward_cell = cell[2*layer+1]\n",
        "            combined_cell = torch.cat([forward_cell, backward_cell], dim=1)\n",
        "            processed_cell = torch.tanh(self.fc_cell(combined_cell))\n",
        "            cell_processed.append(processed_cell)\n",
        "        hidden_final = torch.stack(hidden_processed)\n",
        "        cell_final = torch.stack(cell_processed)\n",
        "        return outputs, hidden_final, cell_final"
      ],
      "metadata": {
        "id": "ycOLGEUjPKcm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention class\n"
      ],
      "metadata": {
        "id": "uSzI72U0nt_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear((enc_hidden_dim * 2) + dec_hidden_dim, dec_hidden_dim)\n",
        "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        src_len = encoder_outputs.size(1)\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        if mask is not None:\n",
        "            attention = attention.masked_fill(mask == 0, -1e10)\n",
        "        attention_weights = F.softmax(attention, dim=1)\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
        "        return context.squeeze(1), attention_weights"
      ],
      "metadata": {
        "id": "iM1HP9d1POjB"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder class\n"
      ],
      "metadata": {
        "id": "87izkYBXnvg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hidden_dim, dec_hidden_dim, n_layers, attention, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim + (enc_hidden_dim * 2), dec_hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0, batch_first=True)\n",
        "        self.fc_out = nn.Linear((enc_hidden_dim * 2) + dec_hidden_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs, mask=None):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        context, attn_weights = self.attention(hidden[-1], encoder_outputs, mask)\n",
        "        context = context.unsqueeze(1)\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)\n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        output = output.squeeze(1)\n",
        "        context = context.squeeze(1)\n",
        "        embedded = embedded.squeeze(1)\n",
        "        prediction = self.fc_out(torch.cat((output, context, embedded), dim=1))\n",
        "        return prediction, hidden, cell, attn_weights"
      ],
      "metadata": {
        "id": "WaeAWUPDPRp7"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq class\n"
      ],
      "metadata": {
        "id": "zjCl6LndnxR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).float()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, src_lengths, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "        src_mask = self.create_mask(src)\n",
        "        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n",
        "        input = trg[:, 0]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs, src_mask)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "vQHq_O4_PT_S"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize model parameters\n"
      ],
      "metadata": {
        "id": "M6xhbITgny_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(en_vocab)\n",
        "OUTPUT_DIM = len(de_vocab)\n",
        "ENC_EMB_DIM = 512\n",
        "DEC_EMB_DIM = 512\n",
        "ENC_HIDDEN_DIM = 512\n",
        "DEC_HIDDEN_DIM = 512\n",
        "ENC_LAYERS = 2\n",
        "DEC_LAYERS = 2\n",
        "ENC_DROPOUT = 0.3\n",
        "DEC_DROPOUT = 0.3\n",
        "SRC_PAD_IDX = en_vocab['<PAD>']\n",
        "TRG_PAD_IDX = de_vocab['<PAD>']"
      ],
      "metadata": {
        "id": "E7_k9I_9PWNp"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize encoder, decoder, and seq2seq model\n"
      ],
      "metadata": {
        "id": "fQbq5t3Pn0mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HIDDEN_DIM, ENC_LAYERS, ENC_DROPOUT)\n",
        "attn = Attention(ENC_HIDDEN_DIM, DEC_HIDDEN_DIM)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HIDDEN_DIM, DEC_HIDDEN_DIM, DEC_LAYERS, attn, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
      ],
      "metadata": {
        "id": "BhybFTTIPYVR"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize model weights\n"
      ],
      "metadata": {
        "id": "8rX0VQlfn2YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.orthogonal_(param.data)\n",
        "        elif 'bias' in name:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eJYV9vvPaiB",
        "outputId": "d5d5735e-f118-499d-883d-02f45d17be57"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7000, 512)\n",
              "    (rnn): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
              "    (fc_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (fc_cell): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(7000, 512)\n",
              "    (rnn): LSTM(1536, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
              "    (fc_out): Linear(in_features=2048, out_features=7000, bias=True)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count trainable parameters\n"
      ],
      "metadata": {
        "id": "_GPVJW_Xn4ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BAICxMuPfRt",
        "outputId": "af96960c-5d44-4069-b7e0-865a46b0e9d3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 40,149,848 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define optimizer and loss function\n"
      ],
      "metadata": {
        "id": "n_QRjs6mn6KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ],
      "metadata": {
        "id": "Nj33tEYdPhu2"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training function\n"
      ],
      "metadata": {
        "id": "wJ7_aepvn7mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(tqdm(iterator, desc=\"Training\")):\n",
        "        src, trg, src_len, _ = batch\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        src_len = src_len.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, src_len, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].contiguous().view(-1, output_dim)\n",
        "        trg = trg[:, 1:].contiguous().view(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "SmY0iDGzPimI"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation function\n"
      ],
      "metadata": {
        "id": "5Sz-Z2xmn9Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(iterator, desc=\"Evaluating\")):\n",
        "            src, trg, src_len, _ = batch\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            src_len = src_len.to(device)\n",
        "            output = model(src, src_len, trg, 0)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].contiguous().view(-1, output_dim)\n",
        "            trg = trg[:, 1:].contiguous().view(-1)\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "IDCYWhaYPllc"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation function\n"
      ],
      "metadata": {
        "id": "EuxRfik9n-mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, sentence, src_vocab, tgt_vocab, src_tokenizer, device, max_len=50):\n",
        "    model.eval()\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = src_tokenizer(sentence)\n",
        "    else:\n",
        "        tokens = sentence\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    src_indices = [src_vocab.get(token, src_vocab['<UNK>']) for token in tokens]\n",
        "    src_indices = [src_vocab['<SOS>']] + src_indices + [src_vocab['<EOS>']]\n",
        "    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n",
        "    src_len = torch.LongTensor([len(src_indices)]).to(device)\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden, cell = model.encoder(src_tensor, src_len)\n",
        "    trg_indices = [tgt_vocab['<SOS>']]\n",
        "    attention_scores = []\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indices[-1]]).to(device)\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell, attention = model.decoder(trg_tensor, hidden, cell, encoder_outputs, mask=model.create_mask(src_tensor))\n",
        "        attention_scores.append(attention.detach().cpu().numpy())\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indices.append(pred_token)\n",
        "        if pred_token == tgt_vocab['<EOS>']:\n",
        "            break\n",
        "    trg_tokens = []\n",
        "    for idx in trg_indices[1:]:\n",
        "        if idx == tgt_vocab['<EOS>']:\n",
        "            break\n",
        "        for token, index in tgt_vocab.items():\n",
        "            if index == idx:\n",
        "                trg_tokens.append(token)\n",
        "                break\n",
        "    return trg_tokens, attention_scores"
      ],
      "metadata": {
        "id": "TQzRSln4PnuR"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU score calculation function\n"
      ],
      "metadata": {
        "id": "jnrEiokLoBqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu(model, data_loader, src_vocab, tgt_vocab, src_tokenizer):\n",
        "    model.eval()\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    for batch in tqdm(data_loader, desc=\"Calculating BLEU\"):\n",
        "        src, trg, src_len, _ = batch\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        src_len = src_len.to(device)\n",
        "        for i in range(len(src)):\n",
        "            src_sentence = []\n",
        "            for idx in src[i]:\n",
        "                idx_item = idx.item()\n",
        "                if idx_item != src_vocab['<PAD>'] and idx_item != src_vocab['<SOS>'] and idx_item != src_vocab['<EOS>']:\n",
        "                    for token, index in src_vocab.items():\n",
        "                        if index == idx_item:\n",
        "                            src_sentence.append(token)\n",
        "                            break\n",
        "            trg_sentence = []\n",
        "            for idx in trg[i]:\n",
        "                idx_item = idx.item()\n",
        "                if idx_item != tgt_vocab['<PAD>'] and idx_item != tgt_vocab['<SOS>'] and idx_item != tgt_vocab['<EOS>']:\n",
        "                    for token, index in tgt_vocab.items():\n",
        "                        if index == idx_item:\n",
        "                            trg_sentence.append(token)\n",
        "                            break\n",
        "            translation, _ = translate_sentence(model, src_sentence, src_vocab, tgt_vocab, src_tokenizer, device)\n",
        "            references.append([\" \".join(trg_sentence)])\n",
        "            hypotheses.append(\" \".join(translation))\n",
        "    bleu = sacrebleu.corpus_bleu(hypotheses, list(zip(*references)))\n",
        "    return bleu.score"
      ],
      "metadata": {
        "id": "5ZrZiW77PqPv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop\n"
      ],
      "metadata": {
        "id": "p8kXMOf0oDSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, n_epochs=20, clip=1.0):\n",
        "    best_valid_loss = float('inf')\n",
        "    best_bleu_score = 0\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch: {epoch+1}/{n_epochs}\")\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, clip)\n",
        "        valid_loss = evaluate(model, val_loader, criterion)\n",
        "        scheduler.step(valid_loss)\n",
        "        if epoch % 3 == 0 or epoch == n_epochs - 1:\n",
        "            subset_size = min(len(val_loader.dataset), 500)\n",
        "            subset_indices = random.sample(range(len(val_loader.dataset)), subset_size)\n",
        "            subset_data = [val_loader.dataset.data_pairs[i] for i in subset_indices]\n",
        "            subset_dataset = TranslationDataset(subset_data, en_vocab, de_vocab, tokenize_en, tokenize_de)\n",
        "            subset_loader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "            bleu_score = calculate_bleu(model, subset_loader, en_vocab, de_vocab, tokenize_en)\n",
        "            print(f\"BLEU Score: {bleu_score:.2f}\")\n",
        "            if bleu_score > best_bleu_score:\n",
        "                best_bleu_score = bleu_score\n",
        "                patience_counter = 0\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'bleu': bleu_score,\n",
        "                    'loss': valid_loss,\n",
        "                }, 'best_model_bleu.pt')\n",
        "                print(f\"Model saved with BLEU: {bleu_score:.2f}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "        else:\n",
        "            if valid_loss < best_valid_loss:\n",
        "                best_valid_loss = valid_loss\n",
        "                patience_counter = 0\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': valid_loss,\n",
        "                }, 'best_model_loss.pt')\n",
        "                print(f\"Model saved with loss: {valid_loss:.4f}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val. Loss: {valid_loss:.4f}\")\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "    return model"
      ],
      "metadata": {
        "id": "5_O_nHyTPsr_"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test translation function\n"
      ],
      "metadata": {
        "id": "rT1eEo7goE90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_translation(model, sentence, src_vocab, tgt_vocab, src_tokenizer):\n",
        "    translated_tokens, attention = translate_sentence(model, sentence, src_vocab, tgt_vocab, src_tokenizer, device)\n",
        "    translated_sentence = \" \".join(translated_tokens)\n",
        "    print(f\"Source: {sentence}\")\n",
        "    print(f\"Translated: {translated_sentence}\")\n",
        "    return translated_sentence"
      ],
      "metadata": {
        "id": "LExi3g47Pxeb"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main execution\n"
      ],
      "metadata": {
        "id": "omdtw9NqoGld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    N_EPOCHS = 20\n",
        "    CLIP = 1.0\n",
        "    model.apply(init_weights)\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "    print(\"Starting training...\")\n",
        "    model = train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, n_epochs=N_EPOCHS, clip=CLIP)\n",
        "    checkpoint = torch.load('best_model_bleu.pt', map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded best model with BLEU score: {checkpoint['bleu']:.2f}\")\n",
        "    val_bleu = calculate_bleu(model, val_loader, en_vocab, de_vocab, tokenize_en)\n",
        "    print(f\"Validation BLEU score: {val_bleu:.2f}\")\n",
        "    test_sentences = [\n",
        "        \"This is a wonderful day to test translation.\",\n",
        "        \"I love learning about natural language processing.\",\n",
        "        \"The book is on the table.\",\n",
        "        \"She walked to the store yesterday.\",\n",
        "        \"Can you help me translate this sentence?\"\n",
        "    ]\n",
        "    print(\"\\nSample translations:\")\n",
        "    for sentence in test_sentences:\n",
        "        translated = test_translation(model, sentence, en_vocab, de_vocab, tokenize_en)\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'en_vocab': en_vocab,\n",
        "        'de_vocab': de_vocab,\n",
        "        'model_params': {\n",
        "            'INPUT_DIM': INPUT_DIM,\n",
        "            'OUTPUT_DIM': OUTPUT_DIM,\n",
        "            'ENC_EMB_DIM': ENC_EMB_DIM,\n",
        "            'DEC_EMB_DIM': DEC_EMB_DIM,\n",
        "            'ENC_HIDDEN_DIM': ENC_HIDDEN_DIM,\n",
        "            'DEC_HIDDEN_DIM': DEC_HIDDEN_DIM,\n",
        "            'ENC_LAYERS': ENC_LAYERS,\n",
        "            'DEC_LAYERS': DEC_LAYERS,\n",
        "            'ENC_DROPOUT': ENC_DROPOUT,\n",
        "            'DEC_DROPOUT': DEC_DROPOUT\n",
        "        }\n",
        "    }, 'translation_model_complete.pt')\n",
        "    print(\"Model saved as 'translation_model_complete.pt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIq8gCRkP0BX",
        "outputId": "a607eb9d-63a9-4b95-aabc-bc924b248ef0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 40,149,848 trainable parameters\n",
            "Starting training...\n",
            "Epoch: 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 456/456 [07:27<00:00,  1.02it/s]\n",
            "Evaluating: 100%|██████████| 46/46 [00:18<00:00,  2.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retained 500 of 500 examples after length filtering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating BLEU: 100%|██████████| 8/8 [00:16<00:00,  2.11s/it]\n",
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 8.76\n",
            "Model saved with BLEU: 8.76\n",
            "Train Loss: 5.4909 | Val. Loss: 5.5241\n",
            "Epoch: 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 456/456 [07:30<00:00,  1.01it/s]\n",
            "Evaluating: 100%|██████████| 46/46 [00:17<00:00,  2.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved with loss: 5.1866\n",
            "Train Loss: 4.8520 | Val. Loss: 5.1866\n",
            "Epoch: 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 456/456 [07:29<00:00,  1.01it/s]\n",
            "Evaluating: 100%|██████████| 46/46 [00:17<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved with loss: 5.0397\n",
            "Train Loss: 4.4007 | Val. Loss: 5.0397\n",
            "Epoch: 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 456/456 [07:33<00:00,  1.01it/s]\n",
            "Evaluating: 100%|██████████| 46/46 [00:17<00:00,  2.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retained 500 of 500 examples after length filtering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating BLEU: 100%|██████████| 8/8 [00:15<00:00,  2.00s/it]\n",
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 15.42\n",
            "Model saved with BLEU: 15.42\n",
            "Train Loss: 4.0427 | Val. Loss: 4.9944\n",
            "Epoch: 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 456/456 [07:29<00:00,  1.01it/s]\n",
            "Evaluating: 100%|██████████| 46/46 [00:17<00:00,  2.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved with loss: 5.0298\n",
            "Train Loss: 3.7239 | Val. Loss: 5.0298\n",
            "Epoch: 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 456/456 [07:32<00:00,  1.01it/s]\n",
            "Evaluating: 100%|██████████| 46/46 [00:17<00:00,  2.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.4532 | Val. Loss: 5.0555\n",
            "Epoch: 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 456/456 [07:30<00:00,  1.01it/s]\n",
            "Evaluating: 100%|██████████| 46/46 [00:17<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retained 500 of 500 examples after length filtering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating BLEU: 100%|██████████| 8/8 [00:16<00:00,  2.07s/it]\n",
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 16.69\n",
            "Model saved with BLEU: 16.69\n",
            "Train Loss: 3.1829 | Val. Loss: 5.1463\n",
            "Epoch: 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 456/456 [07:31<00:00,  1.01it/s]\n",
            "Evaluating: 100%|██████████| 46/46 [00:17<00:00,  2.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.8289 | Val. Loss: 5.1785\n",
            "Epoch: 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 456/456 [07:29<00:00,  1.01it/s]\n",
            "Evaluating: 100%|██████████| 46/46 [00:17<00:00,  2.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.6342 | Val. Loss: 5.3020\n",
            "Epoch: 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 456/456 [07:30<00:00,  1.01it/s]\n",
            "Evaluating: 100%|██████████| 46/46 [00:17<00:00,  2.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retained 500 of 500 examples after length filtering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating BLEU: 100%|██████████| 8/8 [00:16<00:00,  2.08s/it]\n",
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 16.31\n",
            "Train Loss: 2.5149 | Val. Loss: 5.3401\n",
            "Early stopping at epoch 10\n",
            "Loaded best model with BLEU score: 16.69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating BLEU: 100%|██████████| 46/46 [01:43<00:00,  2.26s/it]\n",
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation BLEU score: 16.14\n",
            "\n",
            "Sample translations:\n",
            "Source: This is a wonderful day to test translation.\n",
            "Translated: das ist eine <UNK> tag , <UNK> .\n",
            "Source: I love learning about natural language processing.\n",
            "Translated: ich liebe ich sehr <UNK> <UNK> sprache <UNK> .\n",
            "Source: The book is on the table.\n",
            "Translated: der buch steht auf tische .\n",
            "Source: She walked to the store yesterday.\n",
            "Translated: sie ging zu , , die <UNK> .\n",
            "Source: Can you help me translate this sentence?\n",
            "Translated: ihr ihr mich dieser <UNK> <UNK> ?\n",
            "Model saved as 'translation_model_complete.pt'\n"
          ]
        }
      ]
    }
  ]
}